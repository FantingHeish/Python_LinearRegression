# -*- coding: utf-8 -*-
"""Linear Regression with Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LjtyLPVW8CKivd7Nik_TehU1B9YsbwGI
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import numpy as np
import matplotlib.pyplot as plt

def generate_examples(num=1000):
    W = [1.0, -3.0]
    b = 1.0

    W = np.reshape(W, (2, 1))

    X = np.random.randn(num, 2)

    y = b + np.dot(X, W) + np.random.randn()

    y = np.reshape(y, (num, 1))

    return X, y

X, y = generate_examples()
print(X.shape, y.shape)
print(X[0], y[0])

class Model:
    def __init__(self, num_features):
        self.num_features = num_features
        self.W = np.random.randn(num_features, 1)
        self.b = np.random.randn()

model = Model(2)
print(model.W)
print(model.b)


class Model(Model):
    def forward_pass(self, X):
        y = self.b + np.dot(X, self.W)
        return y

y_hat = Model(2).forward_pass(X)
print(y_hat.shape, y_hat[0])

class Model(Model):
    def compute_loss(self, y, y_true):
        loss = np.sum(np.square(y - y_true))
        return loss/(2*y.shape[0])

model = Model(2)
y_hat = model.forward_pass(X)
loss = model.compute_loss(y_hat, y)

loss


class Model(Model):
      def backward_pass(self, X, y_true, y_hat):
        m = y_hat.shape[0]
        db = np.sum(y_hat - y_true)/m
        dW = np.sum(np.dot(np.transpose(y_hat - y_true), X), axis=0)/m
        return dW, db

model = Model(2)

X, y = generate_examples()
y_hat = model.forward_pass(X)

dW, db =model.backward_pass(X, y, y_hat)

print(dW, db)

class Model(Model):
      def update_params(self, dW, db, lr):
        self.W = self.W - lr * np.reshape(dW, (self.num_features, 1))
        self.b = self.b - lr * db

class Model(Model):
      def train(self, x_train, y_train, iterations, lr):
        losses = []
        for i in range(iterations):
            y_hat = self.forward_pass(x_train)
            dW, db = self.backward_pass(x_train, y_train, y_hat)
            self.update_params(dW, db, lr)
            loss = self.compute_loss(y_hat, y_train)
            losses.append(loss)
            if i % int(iterations/10) == 0:
                print('Iter: {}, Current loss: {:.4f}'.format(i, loss))
        return losses

model = Model(2)
x_train, y_train = generate_examples()
losses = model.train(x_train, y_train, 1000, 3e-3)
plt.plot(losses);

model_untrained = Model(2)

x_test, y_test = generate_examples(500)
print(x_test.shape, y_test.shape)

preds_untrained = model_untrained.forward_pass(x_test)
preds_trained = model.forward_pass(x_test)

plt.figure(figsize=(6, 6))
plt.plot(preds_untrained, y_test, 'rx', label='untrained')
plt.plot(preds_trained, y_test, 'b', label='Trained')
plt.legend()
plt.xlabel('Predictions')
plt.ylabel('Ground Truth')
plt.show()

# ----------------------------
# ----------------------------


# %% time
# %matplotlib inline

# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt

#from linreg import LinearModel

# df = pd.read_excel('chirps.xls')
# df.head()

# x = df.X.values
# y = df.Y.values
# x = np.reshape(x, (x.shape[0], 1))
# y = np.reshape(y, (y.shape[0], 1))

#model = LinearModel(1)

# losses = model.train(
#    x, y,
#    200, 0.0001
#)
#plt.plot(losses);
#y_preds = model.forward_pass(x)
#plt.plot(y, y_preds, 'r.');
#print(model.W, model.b)
#y = 4.75*x + 0.43